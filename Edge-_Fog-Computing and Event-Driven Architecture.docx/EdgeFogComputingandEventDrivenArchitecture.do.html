<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_list_20-2{list-style-type:none}ul.lst-kix_list_20-3{list-style-type:none}.lst-kix_list_19-0>li:before{content:"\0025cf   "}.lst-kix_list_19-1>li:before{content:"\0025cb   "}ul.lst-kix_list_20-4{list-style-type:none}.lst-kix_list_14-1>li:before{content:"\0025cb   "}.lst-kix_list_14-3>li:before{content:"\0025cf   "}ul.lst-kix_list_20-5{list-style-type:none}ul.lst-kix_list_20-6{list-style-type:none}ul.lst-kix_list_1-0{list-style-type:none}ul.lst-kix_list_20-7{list-style-type:none}ul.lst-kix_list_20-8{list-style-type:none}.lst-kix_list_14-0>li:before{content:"\0025cf   "}.lst-kix_list_14-4>li:before{content:"\0025cb   "}.lst-kix_list_19-4>li:before{content:"\0025cb   "}.lst-kix_list_14-5>li:before{content:"\0025a0   "}.lst-kix_list_14-7>li:before{content:"\0025cb   "}.lst-kix_list_19-2>li:before{content:"\0025a0   "}.lst-kix_list_19-3>li:before{content:"\0025cf   "}ul.lst-kix_list_20-0{list-style-type:none}.lst-kix_list_14-6>li:before{content:"\0025cf   "}ul.lst-kix_list_20-1{list-style-type:none}ul.lst-kix_list_9-3{list-style-type:none}ul.lst-kix_list_9-4{list-style-type:none}ul.lst-kix_list_9-1{list-style-type:none}ul.lst-kix_list_9-2{list-style-type:none}ul.lst-kix_list_9-7{list-style-type:none}ul.lst-kix_list_9-8{list-style-type:none}ul.lst-kix_list_9-5{list-style-type:none}ul.lst-kix_list_9-6{list-style-type:none}ul.lst-kix_list_1-3{list-style-type:none}ul.lst-kix_list_1-4{list-style-type:none}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}ul.lst-kix_list_9-0{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}.lst-kix_list_14-2>li:before{content:"\0025a0   "}ul.lst-kix_list_1-6{list-style-type:none}ul.lst-kix_list_17-1{list-style-type:none}ul.lst-kix_list_17-0{list-style-type:none}ul.lst-kix_list_17-8{list-style-type:none}.lst-kix_list_19-8>li:before{content:"\0025a0   "}ul.lst-kix_list_17-7{list-style-type:none}ul.lst-kix_list_17-6{list-style-type:none}ul.lst-kix_list_17-5{list-style-type:none}ul.lst-kix_list_17-4{list-style-type:none}ul.lst-kix_list_17-3{list-style-type:none}.lst-kix_list_14-8>li:before{content:"\0025a0   "}ul.lst-kix_list_17-2{list-style-type:none}.lst-kix_list_19-5>li:before{content:"\0025a0   "}.lst-kix_list_19-6>li:before{content:"\0025cf   "}.lst-kix_list_19-7>li:before{content:"\0025cb   "}.lst-kix_list_5-0>li:before{content:"\0025cf   "}.lst-kix_list_5-3>li:before{content:"\0025cf   "}.lst-kix_list_5-2>li:before{content:"\0025a0   "}.lst-kix_list_5-1>li:before{content:"\0025cb   "}.lst-kix_list_5-7>li:before{content:"\0025cb   "}ul.lst-kix_list_8-4{list-style-type:none}ul.lst-kix_list_8-5{list-style-type:none}.lst-kix_list_5-6>li:before{content:"\0025cf   "}.lst-kix_list_5-8>li:before{content:"\0025a0   "}ul.lst-kix_list_8-2{list-style-type:none}ul.lst-kix_list_8-3{list-style-type:none}ul.lst-kix_list_8-8{list-style-type:none}ul.lst-kix_list_8-6{list-style-type:none}ul.lst-kix_list_8-7{list-style-type:none}.lst-kix_list_5-4>li:before{content:"\0025cb   "}.lst-kix_list_5-5>li:before{content:"\0025a0   "}ul.lst-kix_list_8-0{list-style-type:none}ul.lst-kix_list_8-1{list-style-type:none}.lst-kix_list_6-1>li:before{content:"\0025cb   "}.lst-kix_list_6-3>li:before{content:"\0025cf   "}.lst-kix_list_18-0>li:before{content:"\0025cf   "}.lst-kix_list_6-0>li:before{content:"\0025cf   "}.lst-kix_list_6-4>li:before{content:"\0025cb   "}ul.lst-kix_list_16-2{list-style-type:none}ul.lst-kix_list_16-1{list-style-type:none}.lst-kix_list_18-1>li:before{content:"\0025cb   "}.lst-kix_list_18-2>li:before{content:"\0025a0   "}ul.lst-kix_list_16-0{list-style-type:none}.lst-kix_list_6-2>li:before{content:"\0025a0   "}ul.lst-kix_list_16-8{list-style-type:none}ul.lst-kix_list_16-7{list-style-type:none}ul.lst-kix_list_16-6{list-style-type:none}ul.lst-kix_list_16-5{list-style-type:none}ul.lst-kix_list_16-4{list-style-type:none}.lst-kix_list_6-8>li:before{content:"\0025a0   "}ul.lst-kix_list_16-3{list-style-type:none}.lst-kix_list_6-5>li:before{content:"\0025a0   "}.lst-kix_list_6-7>li:before{content:"\0025cb   "}.lst-kix_list_6-6>li:before{content:"\0025cf   "}.lst-kix_list_2-7>li:before{content:"\0025cb   "}.lst-kix_list_7-4>li:before{content:"\0025cb   "}.lst-kix_list_7-6>li:before{content:"\0025cf   "}.lst-kix_list_2-5>li:before{content:"\0025a0   "}.lst-kix_list_7-2>li:before{content:"\0025a0   "}.lst-kix_list_18-6>li:before{content:"\0025cf   "}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}.lst-kix_list_10-1>li:before{content:"\0025cb   "}.lst-kix_list_18-4>li:before{content:"\0025cb   "}.lst-kix_list_18-8>li:before{content:"\0025a0   "}.lst-kix_list_13-7>li:before{content:"\0025cb   "}ul.lst-kix_list_3-1{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}.lst-kix_list_7-8>li:before{content:"\0025a0   "}ul.lst-kix_list_3-0{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_10-7>li:before{content:"\0025cb   "}.lst-kix_list_15-5>li:before{content:"\0025a0   "}.lst-kix_list_10-5>li:before{content:"\0025a0   "}.lst-kix_list_10-3>li:before{content:"\0025cf   "}ul.lst-kix_list_11-7{list-style-type:none}ul.lst-kix_list_11-6{list-style-type:none}.lst-kix_list_4-1>li:before{content:"\0025cb   "}ul.lst-kix_list_11-5{list-style-type:none}ul.lst-kix_list_11-4{list-style-type:none}ul.lst-kix_list_11-3{list-style-type:none}.lst-kix_list_15-7>li:before{content:"\0025cb   "}ul.lst-kix_list_11-2{list-style-type:none}ul.lst-kix_list_11-1{list-style-type:none}ul.lst-kix_list_11-0{list-style-type:none}.lst-kix_list_9-2>li:before{content:"\0025a0   "}ul.lst-kix_list_19-7{list-style-type:none}ul.lst-kix_list_19-6{list-style-type:none}.lst-kix_list_4-3>li:before{content:"\0025cf   "}.lst-kix_list_4-5>li:before{content:"\0025a0   "}ul.lst-kix_list_19-5{list-style-type:none}ul.lst-kix_list_19-4{list-style-type:none}ul.lst-kix_list_19-3{list-style-type:none}ul.lst-kix_list_19-2{list-style-type:none}ul.lst-kix_list_19-1{list-style-type:none}ul.lst-kix_list_11-8{list-style-type:none}.lst-kix_list_20-8>li:before{content:"\0025a0   "}ul.lst-kix_list_19-0{list-style-type:none}.lst-kix_list_15-1>li:before{content:"\0025cb   "}.lst-kix_list_9-0>li:before{content:"\0025cf   "}.lst-kix_list_15-3>li:before{content:"\0025cf   "}ul.lst-kix_list_19-8{list-style-type:none}.lst-kix_list_20-0>li:before{content:"\0025cf   "}.lst-kix_list_9-6>li:before{content:"\0025cf   "}.lst-kix_list_9-4>li:before{content:"\0025cb   "}.lst-kix_list_11-3>li:before{content:"\0025cf   "}.lst-kix_list_20-6>li:before{content:"\0025cf   "}ul.lst-kix_list_2-8{list-style-type:none}.lst-kix_list_12-3>li:before{content:"\0025cf   "}.lst-kix_list_20-4>li:before{content:"\0025cb   "}.lst-kix_list_11-5>li:before{content:"\0025a0   "}.lst-kix_list_12-1>li:before{content:"\0025cb   "}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_20-2>li:before{content:"\0025a0   "}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}.lst-kix_list_9-8>li:before{content:"\0025a0   "}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"\0025cb   "}ul.lst-kix_list_2-7{list-style-type:none}.lst-kix_list_11-7>li:before{content:"\0025cb   "}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}ul.lst-kix_list_10-0{list-style-type:none}.lst-kix_list_1-3>li:before{content:"\0025cf   "}.lst-kix_list_13-3>li:before{content:"\0025cf   "}ul.lst-kix_list_10-8{list-style-type:none}ul.lst-kix_list_18-0{list-style-type:none}ul.lst-kix_list_10-7{list-style-type:none}.lst-kix_list_1-7>li:before{content:"\0025cb   "}ul.lst-kix_list_10-6{list-style-type:none}ul.lst-kix_list_10-5{list-style-type:none}ul.lst-kix_list_10-4{list-style-type:none}ul.lst-kix_list_10-3{list-style-type:none}.lst-kix_list_1-5>li:before{content:"\0025a0   "}ul.lst-kix_list_10-2{list-style-type:none}ul.lst-kix_list_10-1{list-style-type:none}.lst-kix_list_13-5>li:before{content:"\0025a0   "}.lst-kix_list_12-5>li:before{content:"\0025a0   "}ul.lst-kix_list_18-8{list-style-type:none}ul.lst-kix_list_18-7{list-style-type:none}ul.lst-kix_list_18-6{list-style-type:none}ul.lst-kix_list_18-5{list-style-type:none}.lst-kix_list_12-7>li:before{content:"\0025cb   "}ul.lst-kix_list_18-4{list-style-type:none}.lst-kix_list_2-1>li:before{content:"\0025cb   "}ul.lst-kix_list_18-3{list-style-type:none}ul.lst-kix_list_18-2{list-style-type:none}ul.lst-kix_list_18-1{list-style-type:none}.lst-kix_list_2-3>li:before{content:"\0025cf   "}.lst-kix_list_13-1>li:before{content:"\0025cb   "}.lst-kix_list_3-0>li:before{content:"\0025cf   "}ul.lst-kix_list_5-7{list-style-type:none}ul.lst-kix_list_5-8{list-style-type:none}.lst-kix_list_3-1>li:before{content:"\0025cb   "}.lst-kix_list_3-2>li:before{content:"\0025a0   "}ul.lst-kix_list_5-5{list-style-type:none}ul.lst-kix_list_5-6{list-style-type:none}.lst-kix_list_8-1>li:before{content:"\0025cb   "}.lst-kix_list_8-2>li:before{content:"\0025a0   "}.lst-kix_list_3-5>li:before{content:"\0025a0   "}ul.lst-kix_list_5-0{list-style-type:none}.lst-kix_list_3-4>li:before{content:"\0025cb   "}ul.lst-kix_list_5-3{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\0025cf   "}ul.lst-kix_list_5-4{list-style-type:none}ul.lst-kix_list_5-1{list-style-type:none}.lst-kix_list_8-0>li:before{content:"\0025cf   "}ul.lst-kix_list_5-2{list-style-type:none}.lst-kix_list_8-7>li:before{content:"\0025cb   "}.lst-kix_list_3-8>li:before{content:"\0025a0   "}.lst-kix_list_8-5>li:before{content:"\0025a0   "}.lst-kix_list_8-6>li:before{content:"\0025cf   "}.lst-kix_list_8-3>li:before{content:"\0025cf   "}ul.lst-kix_list_13-5{list-style-type:none}ul.lst-kix_list_13-4{list-style-type:none}ul.lst-kix_list_13-3{list-style-type:none}.lst-kix_list_3-6>li:before{content:"\0025cf   "}ul.lst-kix_list_13-2{list-style-type:none}ul.lst-kix_list_13-1{list-style-type:none}.lst-kix_list_3-7>li:before{content:"\0025cb   "}ul.lst-kix_list_13-0{list-style-type:none}.lst-kix_list_8-4>li:before{content:"\0025cb   "}.lst-kix_list_11-2>li:before{content:"\0025a0   "}ul.lst-kix_list_13-8{list-style-type:none}.lst-kix_list_11-1>li:before{content:"\0025cb   "}ul.lst-kix_list_13-7{list-style-type:none}ul.lst-kix_list_13-6{list-style-type:none}.lst-kix_list_11-0>li:before{content:"\0025cf   "}.lst-kix_list_8-8>li:before{content:"\0025a0   "}.lst-kix_list_16-8>li:before{content:"\0025a0   "}.lst-kix_list_16-7>li:before{content:"\0025cb   "}.lst-kix_list_16-6>li:before{content:"\0025cf   "}.lst-kix_list_4-8>li:before{content:"\0025a0   "}.lst-kix_list_4-7>li:before{content:"\0025cb   "}.lst-kix_list_17-0>li:before{content:"\0025cf   "}.lst-kix_list_17-1>li:before{content:"\0025cb   "}ul.lst-kix_list_4-8{list-style-type:none}.lst-kix_list_16-0>li:before{content:"\0025cf   "}ul.lst-kix_list_4-6{list-style-type:none}.lst-kix_list_16-1>li:before{content:"\0025cb   "}ul.lst-kix_list_4-7{list-style-type:none}.lst-kix_list_16-2>li:before{content:"\0025a0   "}ul.lst-kix_list_4-0{list-style-type:none}.lst-kix_list_16-4>li:before{content:"\0025cb   "}ul.lst-kix_list_4-1{list-style-type:none}.lst-kix_list_16-3>li:before{content:"\0025cf   "}.lst-kix_list_16-5>li:before{content:"\0025a0   "}ul.lst-kix_list_4-4{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ul.lst-kix_list_4-2{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}ul.lst-kix_list_12-6{list-style-type:none}ul.lst-kix_list_12-5{list-style-type:none}.lst-kix_list_17-7>li:before{content:"\0025cb   "}ul.lst-kix_list_12-4{list-style-type:none}ul.lst-kix_list_12-3{list-style-type:none}ul.lst-kix_list_12-2{list-style-type:none}ul.lst-kix_list_12-1{list-style-type:none}.lst-kix_list_17-8>li:before{content:"\0025a0   "}ul.lst-kix_list_12-0{list-style-type:none}.lst-kix_list_17-3>li:before{content:"\0025cf   "}.lst-kix_list_17-2>li:before{content:"\0025a0   "}.lst-kix_list_17-4>li:before{content:"\0025cb   "}ul.lst-kix_list_12-8{list-style-type:none}ul.lst-kix_list_12-7{list-style-type:none}.lst-kix_list_17-6>li:before{content:"\0025cf   "}.lst-kix_list_7-0>li:before{content:"\0025cf   "}.lst-kix_list_17-5>li:before{content:"\0025a0   "}.lst-kix_list_2-6>li:before{content:"\0025cf   "}.lst-kix_list_2-4>li:before{content:"\0025cb   "}.lst-kix_list_2-8>li:before{content:"\0025a0   "}.lst-kix_list_7-1>li:before{content:"\0025cb   "}.lst-kix_list_7-5>li:before{content:"\0025a0   "}.lst-kix_list_7-3>li:before{content:"\0025cf   "}ul.lst-kix_list_7-5{list-style-type:none}.lst-kix_list_10-0>li:before{content:"\0025cf   "}ul.lst-kix_list_7-6{list-style-type:none}.lst-kix_list_18-5>li:before{content:"\0025a0   "}ul.lst-kix_list_7-3{list-style-type:none}ul.lst-kix_list_7-4{list-style-type:none}.lst-kix_list_13-6>li:before{content:"\0025cf   "}.lst-kix_list_13-8>li:before{content:"\0025a0   "}.lst-kix_list_18-3>li:before{content:"\0025cf   "}.lst-kix_list_18-7>li:before{content:"\0025cb   "}ul.lst-kix_list_7-7{list-style-type:none}ul.lst-kix_list_7-8{list-style-type:none}ul.lst-kix_list_7-1{list-style-type:none}ul.lst-kix_list_7-2{list-style-type:none}ul.lst-kix_list_7-0{list-style-type:none}.lst-kix_list_7-7>li:before{content:"\0025cb   "}.lst-kix_list_15-4>li:before{content:"\0025cb   "}.lst-kix_list_15-6>li:before{content:"\0025cf   "}.lst-kix_list_10-4>li:before{content:"\0025cb   "}.lst-kix_list_10-8>li:before{content:"\0025a0   "}.lst-kix_list_4-0>li:before{content:"\0025cf   "}ul.lst-kix_list_15-3{list-style-type:none}ul.lst-kix_list_15-2{list-style-type:none}.lst-kix_list_15-0>li:before{content:"\0025cf   "}ul.lst-kix_list_15-1{list-style-type:none}.lst-kix_list_15-8>li:before{content:"\0025a0   "}ul.lst-kix_list_15-0{list-style-type:none}.lst-kix_list_10-2>li:before{content:"\0025a0   "}.lst-kix_list_4-4>li:before{content:"\0025cb   "}.lst-kix_list_20-7>li:before{content:"\0025cb   "}ul.lst-kix_list_15-8{list-style-type:none}.lst-kix_list_4-2>li:before{content:"\0025a0   "}.lst-kix_list_4-6>li:before{content:"\0025cf   "}ul.lst-kix_list_15-7{list-style-type:none}ul.lst-kix_list_15-6{list-style-type:none}.lst-kix_list_9-3>li:before{content:"\0025cf   "}ul.lst-kix_list_15-5{list-style-type:none}ul.lst-kix_list_15-4{list-style-type:none}.lst-kix_list_15-2>li:before{content:"\0025a0   "}.lst-kix_list_10-6>li:before{content:"\0025cf   "}.lst-kix_list_9-1>li:before{content:"\0025cb   "}.lst-kix_list_9-7>li:before{content:"\0025cb   "}.lst-kix_list_11-4>li:before{content:"\0025cb   "}.lst-kix_list_12-4>li:before{content:"\0025cb   "}.lst-kix_list_9-5>li:before{content:"\0025a0   "}ul.lst-kix_list_6-6{list-style-type:none}ul.lst-kix_list_6-7{list-style-type:none}ul.lst-kix_list_6-4{list-style-type:none}.lst-kix_list_20-5>li:before{content:"\0025a0   "}ul.lst-kix_list_6-5{list-style-type:none}ul.lst-kix_list_6-8{list-style-type:none}.lst-kix_list_12-2>li:before{content:"\0025a0   "}.lst-kix_list_11-6>li:before{content:"\0025cf   "}.lst-kix_list_1-0>li:before{content:"\0025cf   "}.lst-kix_list_20-1>li:before{content:"\0025cb   "}.lst-kix_list_20-3>li:before{content:"\0025cf   "}ul.lst-kix_list_6-2{list-style-type:none}.lst-kix_list_11-8>li:before{content:"\0025a0   "}ul.lst-kix_list_6-3{list-style-type:none}.lst-kix_list_1-2>li:before{content:"\0025a0   "}ul.lst-kix_list_6-0{list-style-type:none}.lst-kix_list_12-0>li:before{content:"\0025cf   "}ul.lst-kix_list_6-1{list-style-type:none}.lst-kix_list_1-4>li:before{content:"\0025cb   "}.lst-kix_list_13-0>li:before{content:"\0025cf   "}ul.lst-kix_list_14-4{list-style-type:none}ul.lst-kix_list_14-3{list-style-type:none}ul.lst-kix_list_14-2{list-style-type:none}.lst-kix_list_13-4>li:before{content:"\0025cb   "}ul.lst-kix_list_14-1{list-style-type:none}ul.lst-kix_list_14-0{list-style-type:none}.lst-kix_list_1-6>li:before{content:"\0025cf   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_list_14-8{list-style-type:none}ul.lst-kix_list_14-7{list-style-type:none}.lst-kix_list_2-0>li:before{content:"\0025cf   "}.lst-kix_list_12-6>li:before{content:"\0025cf   "}ul.lst-kix_list_14-6{list-style-type:none}ul.lst-kix_list_14-5{list-style-type:none}.lst-kix_list_1-8>li:before{content:"\0025a0   "}.lst-kix_list_2-2>li:before{content:"\0025a0   "}.lst-kix_list_13-2>li:before{content:"\0025a0   "}.lst-kix_list_12-8>li:before{content:"\0025a0   "}ol{margin:0;padding:0}table td,table th{padding:0}.c2{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c26{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c25{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:16pt;font-family:"Arial";font-style:normal}.c28{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:14pt;font-family:"Arial";font-style:normal}.c19{-webkit-text-decoration-skip:none;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left;height:11pt}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:center;height:11pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{background-color:#ffffff;color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c8{color:#2c2c38;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c29{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:center}.c9{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c18{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:left}.c36{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c22{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:left}.c4{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c30{text-decoration-skip-ink:none;font-size:16pt;-webkit-text-decoration-skip:none;text-decoration:underline}.c34{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c32{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c23{margin-left:36pt;margin-right:36pt}.c5{padding:0;margin:0}.c12{margin-left:72pt;padding-left:0pt}.c13{margin-left:108pt;padding-left:0pt}.c16{color:inherit;text-decoration:inherit}.c37{margin-left:54pt;margin-right:54pt}.c11{margin-left:67.5pt;margin-right:72pt}.c27{margin-left:72pt}.c31{margin-right:36pt}.c20{font-style:italic}.c33{text-indent:36pt}.c35{margin-right:72pt}.c17{color:#2c2c38}.c15{background-color:#ffffff}.c24{margin-left:36pt}.c0{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c15 c32 doc-content"><p class="c29"><span class="c25 c0">Executive Summary</span></p><p class="c7"><span class="c25 c0"></span></p><p class="c10"><span class="c9 c0">Problem:</span></p><p class="c10 c33"><span class="c1">Time series data loses value over time. &nbsp;The very design of InfluxDB is evidence in support of this claim. &nbsp;The earlier data needs processing the more amplified this concept is. &nbsp;In real-time analytics use cases, this value decay applies to data in its transmission to the database as well. &nbsp;As InfluxDB is often the end destination of data, it can necessarily be assumed that data can and often does have its value extracted before InfluxDB sees it.</span></p><p class="c10"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Innovations in stream processing exacerbate this problem by draining the value of the data in the middle of the data path. &nbsp;Monitoring, alerting, and event-driven programs can be run from the streams which leaves storage solutions supporting use cases that are less valuable.</span></p><p class="c3"><span class="c9 c0"></span></p><p class="c10"><span class="c9 c0">Proposal:</span></p><p class="c10"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">InfluxData would offer an open source / open core stack for driving the intelligence of monitoring any and all &ldquo;things&rdquo; as close to the edge as desired. &nbsp;The proposal is to a) add to Telegraf more intelligent configurability that supports more advanced programmatic analytics at the edge, and b) include InfluxDB 2.0 for site-level analytics where larger datasets and multi-device context is needed.</span></p><p class="c3"><span class="c9 c0"></span></p><p class="c10"><span class="c9 c0">Business case:</span></p><p class="c22"><span class="c17 c19">Value to customers</span></p><ul class="c5 lst-kix_list_17-0 start"><li class="c18 li-bullet-0"><span class="c17">Real-time insights for proactive remediation</span></li><li class="c18 li-bullet-0"><span class="c17">Edge-automation to reduce human intervention</span></li><li class="c18 li-bullet-0"><span class="c17">Cost savings over network</span></li><li class="c18 li-bullet-0"><span class="c17">Cost savings in cloud computing</span></li><li class="c18 li-bullet-0"><span class="c17">Cost savings in cloud storage</span></li><li class="c18 li-bullet-0"><span class="c17">Guard against &ldquo;air gaps&rdquo;</span></li><li class="c18 li-bullet-0"><span class="c17">Security -- less data leaving private network</span></li></ul><p class="c22"><span class="c19 c17">Value to InfluxData</span></p><ul class="c5 lst-kix_list_7-0 start"><li class="c18 li-bullet-0"><span class="c17">Take control of data path -- own the beginning and end</span></li><li class="c18 li-bullet-0"><span class="c17">Enables more use cases that were originally too high cardinality</span></li><li class="c18 li-bullet-0"><span class="c17">Commercially offered solutions/&rdquo;presets&rdquo;</span></li><li class="c18 li-bullet-0"><span class="c20 c17">Flux adoption</span></li></ul><p class="c7"><span class="c25 c0"></span></p><p class="c7"><span class="c25 c0"></span></p><p class="c7"><span class="c0 c25"></span></p><p class="c29"><span class="c0 c30">Edge-Computing / EDA Vision</span><span class="c0 c14">&nbsp;(v1.0 draft)</span></p><p class="c3"><span class="c28 c0"></span></p><p class="c10"><span class="c0 c28">Table of contents</span></p><ul class="c5 lst-kix_list_12-0 start"><li class="c2 li-bullet-0"><span class="c4 c0"><a class="c16" href="#id.gjdgxs">Motivation</a></span></li><li class="c2 li-bullet-0"><span class="c4 c0"><a class="c16" href="#id.30j0zll">Feasibility</a></span><span class="c0 c34">&nbsp;</span></li><li class="c2 li-bullet-0"><span class="c4 c0"><a class="c16" href="#id.1fob9te">Business case</a></span></li><li class="c2 li-bullet-0"><span class="c4 c0"><a class="c16" href="#id.3znysh7">Proposal</a></span></li><li class="c2 li-bullet-0"><span class="c4 c0"><a class="c16" href="#id.2et92p0">My opinion on an MVP</a></span></li></ul><p class="c3"><span class="c26 c0"></span></p><p class="c3"><span class="c26 c0"></span></p><a id="id.gjdgxs"></a><p class="c10"><span class="c26 c0">Motivation</span></p><p class="c3"><span class="c26 c0"></span></p><p class="c10"><span class="c9 c0">Market direction</span></p><p class="c10"><span>Edge-computing is a North Star for the IoT world -- in some respects, it already exists in the form of &ldquo;control loops&rdquo; -- and an aspiration of the infrastructure monitoring world. &nbsp;Other groups of people in this world are noticing this as well. &nbsp;</span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://www.researchgate.net/publication/322517123_Predictive_edge_computing_for_time_series_of_industrial_IoT_and_large_scale_critical_infrastructure_based_on_open-source_software_analytic_of_big_data&amp;sa=D&amp;source=editors&amp;ust=1691727433110951&amp;usg=AOvVaw2syxXWsp3HCOAsRNAFFACe">Here is a research paper</a></span><span>&nbsp;that points to the need for platform-based edge-computing. &nbsp; </span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/document/8014357&amp;sa=D&amp;source=editors&amp;ust=1691727433111468&amp;usg=AOvVaw2y_wWvKdz7xGfv5u3ogEly">Here is one</a></span><span>&nbsp;on specifically edge stream processing! &nbsp;</span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://www.machbase.com/dev/2018/05/11/edge-computing-high-performance-time-series-database/&amp;sa=D&amp;source=editors&amp;ust=1691727433111795&amp;usg=AOvVaw3EwV9Ls8u4XiZJ5GZqwOeq">Machbase</a></span><span>&nbsp;-- formerly Infiniflux -- seems to be positioning itself for this. &nbsp;</span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://edgedb.com/&amp;sa=D&amp;source=editors&amp;ust=1691727433112029&amp;usg=AOvVaw2Qz8HFxkCSEYSGM0UiViyH">EdgeDB</a></span><span>&nbsp;(also a </span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/document/8850107&amp;sa=D&amp;source=editors&amp;ust=1691727433112271&amp;usg=AOvVaw1aPVrV3vzFiyxXhuuKTA8u">paper on this</a></span><span class="c1">) is an example of a company trying to do this.</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span>Outside of the data being captured and processed at the edge are innovations around edge infrastructure. &nbsp;For instance, at least two companies are notably trying to take Kubernetes to the edge. &nbsp;</span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://microk8s.io/&amp;sa=D&amp;source=editors&amp;ust=1691727433112777&amp;usg=AOvVaw1Eeww1-PfwjHw-VD8N_Zwi">Canonical&rsquo;s MicroK8s</a></span><span>&nbsp;is aimed at doing this and </span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://k3s.io/&amp;sa=D&amp;source=editors&amp;ust=1691727433113052&amp;usg=AOvVaw2H68qHFIWZJ7ZTQ7FyaImm">Rancher&rsquo;s K3s</a></span><span>&nbsp;is as well. &nbsp;It&rsquo;s also worth noting the advancements made and the trending direction toward &ldquo;hybrid cloud&rdquo; with announcements like Google&rsquo;s Anthos or </span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://www.datacenterknowledge.com/equinix/equinix-s-335m-packet-acquisition-closed-here-s-what-s-next&amp;sa=D&amp;source=editors&amp;ust=1691727433113388&amp;usg=AOvVaw0Ik_VPpQmVPDQXaYNWJnZ7">Equinix&rsquo; acquisition of Packet</a></span><span class="c1">&nbsp;to enable more fluid scaling of their customers&rsquo; hybrid clouds.</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span>Lastly, it&rsquo;s worth noting that there is a significant camp in academia proposing that perfect accuracy can be sacrificed for the benefits of edge computing. &nbsp;</span><span class="c4"><a class="c16" href="https://www.google.com/url?q=http://blinkdb.org/&amp;sa=D&amp;source=editors&amp;ust=1691727433113796&amp;usg=AOvVaw3dNmnOVeAI43B0-iYvtUUp">BlinkDB</a></span><span>&nbsp;out of UC Berkeley is centered around this idea. &nbsp; </span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/2987550.2987580&amp;sa=D&amp;source=editors&amp;ust=1691727433114065&amp;usg=AOvVaw3_QpJ8ynoxiX3GXhT9B-kD">This is a paper</a></span><span>&nbsp;out of University of Minnesota and UMass that discusses how we can implement </span><span class="c20">practical online algorithms</span><span>&nbsp;referencing tested offline algorithms, all in the name of reducing latency while retaining an effective error rate in edge analytics. &nbsp;</span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://sameeragarwal.github.io/mod282-agarwal.pdf&amp;sa=D&amp;source=editors&amp;ust=1691727433114388&amp;usg=AOvVaw1GRNx85sHuMrjZO27yWfDH">A paper</a></span><span class="c1">&nbsp;from UC Berkeley, UMich, and MIT notes that &ldquo;it has been widely observed that many applications can tolerate some degree of inaccuracy&rdquo;.</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span>While we&rsquo;re at it, </span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://dl.acm.org/doi/epdf/10.1145/3392156&amp;sa=D&amp;source=editors&amp;ust=1691727433114805&amp;usg=AOvVaw0O0keftNArg5fl77pAm8SD">here&rsquo;s another supporting paper</a></span><span class="c1">&nbsp;published June 2020!</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c1">It&rsquo;s also important to keep in mind that -- in addition to the academic support this idea has -- most time series analysis is prone to some degree of error to begin with. &nbsp;Aggregations, dropped metrics, coefficient omissions in ML, etc., all lead to loss of information in some form or another.</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c1">I believe InfluxData is poised to take advantage of the still rather greenfield opportunity of processing time series data at the edge.</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">What can edge-computing accomplish?</span></p><ul class="c5 lst-kix_list_14-0 start"><li class="c2 li-bullet-0"><span>Low latency and data transit costs of event-driven decisions -- no more waiting for data movement, storage, and then query</span></li></ul><ul class="c5 lst-kix_list_14-1 start"><li class="c10 c12 li-bullet-0"><span>Alerts</span></li><li class="c10 c12 li-bullet-0"><span>Autoscaling events</span></li><li class="c10 c12 li-bullet-0"><span>Reactions to external factors like internet outages between edge and cloud</span></li></ul><ul class="c5 lst-kix_list_14-0"><li class="c2 li-bullet-0"><span>Pre-aggregation of data streams</span></li></ul><ul class="c5 lst-kix_list_14-1 start"><li class="c10 c12 li-bullet-0"><span>Downsampling (multiple write streams)</span></li><li class="c10 c12 li-bullet-0"><span>Pre-computing of aggregations users were going to do at query time anyways, like percentiles, rates, means, mins, maxes, etc.</span></li></ul><ul class="c5 lst-kix_list_14-0"><li class="c2 li-bullet-0"><span>Conditional routing of data</span></li><li class="c2 li-bullet-0"><span>Conditional downsampling and other behavior</span></li><li class="c2 li-bullet-0"><span>Local dashboarding (exists in some form now but can&rsquo;t be forgotten)</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">Some current edge-computing examples:</span></p><ul class="c5 lst-kix_list_9-0 start"><li class="c2 li-bullet-0"><span class="c0">Google Photos stabilization: &nbsp;</span><span>The Google Photos app (comes with Android and free on Apple Store) has the ability to stabilize moving images. &nbsp;It does this with machine learning. &nbsp;It does this with machine learning </span><span class="c20">on your phone</span><span>. &nbsp;That&rsquo;s the edge. &nbsp;The amount of data needed to train and test a model to do moving image stabilization is absolutely infeasible for a phone to support...but the model can be trained elsewhere...and deployed to the phones. &nbsp;That&rsquo;s exactly what happens. &nbsp;The petabyte-scale data processing conducted for training a stabilization model is distributed across huge clusters of compute engines but the trained algorithm, itself, is shipped as part of the Photos app.</span></li><li class="c2 li-bullet-0"><span class="c0">Google Photos like-images: &nbsp;</span><span>Photos also has the ability to group similar photos to suggest grouping them into an album. &nbsp;This also happens at the device level.</span></li><li class="c2 li-bullet-0"><span class="c0">IoT control loop: </span><span class="c15">&nbsp;Monitoring the health/performance of sensors and meters is one thing but remediating issues or making optimizations automatically, without human intervention, is significantly more valuable. &nbsp;This quote from an IEEE paper sums it pretty nicely: </span></li></ul><p class="c10 c37"><span class="c21 c20 c15">Internet of Things envisions a self-configuring, adaptive, complex network that interconnects &#39;things&#39; to the Internet through the use of standard communication protocols. &hellip; The things offer services, with or without human intervention, through the exploitation of unique identification, data capture and communication, and actuation capability.</span></p><p class="c10 c23"><span class="c1 c15">OSIsoft is a major player in this vertical. &nbsp;I&rsquo;m unfamiliar with the detail on this but I&rsquo;m confident that its closed/proprietary nature makes for rigid tailoring of the types of remediations operators likely want or need to do.</span></p><ul class="c5 lst-kix_list_9-0"><li class="c2 c31 li-bullet-0"><span class="c15 c0">Sensu and Nagios: </span><span class="c15">Nagios is the legacy version of Sensu&rsquo;s more modern day approach to alerting at the edge. &nbsp;These technologies run &ldquo;checks&rdquo; against systems at the edge and evaluate the need for alerts based on the results of those checks. &nbsp;In other words, before waiting to query a database, they alert in real-time at the edge. &nbsp;</span></li></ul><ul class="c5 lst-kix_list_9-1 start"><li class="c10 c12 c31 li-bullet-0"><span class="c15 c0">Drawbacks: </span><span class="c15">These might be obvious. &nbsp;Alerting at the edge provides no historical view. &nbsp;You can&rsquo;t run a dashboard off of it. &nbsp;You can&rsquo;t apply context. &nbsp;False negatives and false positives are classically common as a result of a lot of this. &nbsp;Also, alerts aren&rsquo;t the only thing you want out of monitoring. &nbsp;Simply being able to observe your assets as they work comes with its own value...and this can&rsquo;t be done with edge checks on their own. &nbsp;So why not do both...while making the edge aspect smarter? &nbsp;</span></li></ul><ul class="c5 lst-kix_list_9-0"><li class="c2 li-bullet-0"><span class="c0">High frequency trading</span><span>: While this one may not be one we take part in (not low level / fast enough), it&rsquo;s analogous to the theme. &nbsp;These firms care about every microsecond so they have no choice but to make decisions on what to trade as soon as the data that drives those decisions becomes available. &nbsp;Waiting for data to traverse the network, write to a data store, and be analyzed from there...would put them out of business. &nbsp;Like the Google Photos example, they train their trading models on larger infrastructure and ship the ready models to the edge to make gametime decisions.</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><a id="id.30j0zll"></a><p class="c10"><span class="c26 c0">Feasibility</span></p><p class="c10"><span class="c1">I believe current lack of progress in this realm is less of an indicator and more of an opportunity for Influx. &nbsp;That said, I&rsquo;ll start this section by naming some possible reasons why now is the time as opposed to previously:</span></p><ul class="c5 lst-kix_list_8-0 start"><li class="c2 li-bullet-0"><span>Available supporting infrastructure has relatively recently become efficient and uniform enough to enable this type of work. to host computing applications/services. &nbsp;The edge is often now comprised of lightweight POSIX-based OSes that can be instrumented with ease. &nbsp;Also, there is more and more tooling developed to make computation far easier to program. &nbsp;</span><span class="c20 c0">More on this and how it relates to us later</span><span class="c20">.</span></li><li class="c2 li-bullet-0"><span class="c1">Edge-computing inherently requires having distributed and isolated computation apps/services. &nbsp;The meaning of &ldquo;distributed&rdquo; here matters to understand why this is not actually the challenge one might, at first, think it is. &nbsp;While the computations are taking place physically separate and on entirely separate datasets, they&rsquo;re not meant to know about each other. &nbsp;The computations that would be done here would have all the information they need within their own [micro-]environment. &nbsp;Context across distributed systems only matters in cloud-computing and that would come later. &nbsp;Any insights that can be gleaned given only the data available locally...should be, and that should happen as quickly and as close to that data as possible. &nbsp;Even in cases where further context is needed, &ldquo;the edge&rdquo; really isn&rsquo;t black and white. &nbsp;It&rsquo;s not &ldquo;either you&rsquo;re computing at the most primitive data source or you&rsquo;re not edge-computing&rdquo;. &nbsp;Edge-computing can happen at different &ldquo;tiers&rdquo; as well. &nbsp;These &ldquo;tiers&rdquo; could take the form of, say: edge devices are tier 0, edge clusters are tier 1, pipeline to the cloud is tier2, and cloud is tier3.</span></li><li class="c2 li-bullet-0"><span class="c0">More reliable </span><span class="c4 c0"><a class="c16" href="https://www.google.com/url?q=https://docs.confluent.io/current/streams/concepts.html%23duality-of-streams-and-tables&amp;sa=D&amp;source=editors&amp;ust=1691727433118363&amp;usg=AOvVaw0XSTjAwmjVpADFxr-w--Ml">stream-table duality</a></span><span>. &nbsp;Edge makes joining tables (state) with streams less volatile and error-prone. &nbsp;With datasets downstream and in databases, ingest is less predictable. &nbsp;Therefore, use cases where, say, you may want to do streaming analysis on certain datasets/keys and batch analysis on others, are not reliable anywhere but at the edge where volatility of workload is at its lowest.</span></li><li class="c2 li-bullet-0"><span>There is a spectrum of what should and should not be edge-computed, fog-computed, and cloud-computed. &nbsp;The paper from University of Minnesota cited toward the beginning states this flexibility nicely, stemming from their own experience:</span></li></ul><p class="c3 c24"><span class="c1"></span></p><p class="c10 c23 c33"><span class="c20">...it is not always feasible to compute exact results with bounded staleness. Further, many real-world applications can tolerate some staleness or inaccuracy in their final results, albeit with diverse preferences. For instance, a network administrator may need to be alerted to potential network overloads quickly (within a few seconds or minutes), even if there is some degree of error in the results describing network load. On the other hand, a Web analyst might have only a small tolerance for error (say, &lt;1%) in the application statistics (e.g., number of page hits), but be willing to wait for some time to obtain these results with the desired accuracy.</span><span class="c1">&nbsp;</span></p><p class="c3 c23"><span class="c1"></span></p><p class="c10 c23"><span class="c1">It is the job of this model to enable both use cases to be met successfully in the same infrastructure.</span></p><p class="c3 c31"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><a id="id.1fob9te"></a><p class="c10"><span class="c26 c0">Business case</span></p><p class="c3"><span class="c26 c0"></span></p><p class="c10"><span class="c1">Let&rsquo;s discuss why edge-computing is such a sought-after concept:</span></p><ul class="c5 lst-kix_list_10-0 start"><li class="c2 li-bullet-0"><span>Faster insights -- true proactivity -- as close to real-time as one can pragmatically get</span></li><li class="c2 li-bullet-0"><span>Cost savings on data transfer / network</span></li><li class="c2 li-bullet-0"><span>Cost savings on central infrastructure (cloud computing)</span></li></ul><ul class="c5 lst-kix_list_10-1 start"><li class="c10 c12 li-bullet-0"><span>from &ldquo;big data&rdquo; to not so big</span></li><li class="c10 c12 li-bullet-0"><span>there will be a lot of use cases for having all the data stored centrally still but, at the very least, the need for processing/computation on that data will be reduced</span></li></ul><ul class="c5 lst-kix_list_10-0"><li class="c2 li-bullet-0"><span>More secure as processing is happening on customer premises and is subject to all of the same security their assets are.</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">Why InfluxData is the right company to tackle this problem:</span></p><ul class="c5 lst-kix_list_5-0 start"><li class="c2 li-bullet-0"><span>InfluxData historically deals with time series analytics better than most others. &nbsp;InfluxData also created and maintains one of the most popular ways of getting time series data from the edge to the core. &nbsp;This combination is conceptually a perfect starting point for edge-/fog-computing. &nbsp;Imagine Telegraf doing the computations that can and should be done at the edge, computation that needs slightly larger datasets or more context being done on an InfluxDB node, and then all the results or any other raw data being forwarded on to the core for higher level observability.</span></li><li class="c2 li-bullet-0"><span>In most cases, these computations are of a time series nature and we are home to a lot of expertise in that field.</span></li><li class="c2 li-bullet-0"><span>Event / point-in-time data is often irregular and requires a push-based model. &nbsp;Exporting irregular data to be scraped later (Prometheus&rsquo; PushGateway approach) doesn&rsquo;t provide the same real-time value as a true push-based model.</span></li><li class="c2 li-bullet-0"><span>By offering more analytical capabilities in an edge service that pushes results out, you reduce the value of an incumbent Prometheus server.</span></li></ul><p class="c10 c24"><span class="c21 c20">You may think that discussing specifically Prometheus is too narrow in scope. &nbsp;My argument against that is that it really is the de facto Kubernetes monitoring solution...and Kubernetes has already won the consolidation game. &nbsp;It is the future of infrastructure and being THE solution for monitoring it is obviously one of the more impactful strategies we can follow. &nbsp;Let&rsquo;s not forget that Docker was poised to take the container orchestration market by storm by adopting Kubernetes as a first class citizen right away. &nbsp;They didn&rsquo;t and their business has failed.</span></p><ul class="c5 lst-kix_list_5-0"><li class="c2 li-bullet-0"><span>Influx already has a very popular agent </span><span>that has a plugin-/python-based (embedded) computation method</span><span>. &nbsp;This allows for higher adoption out of the gate...which leads to more user feedback and more noise for the brand.</span></li><li class="c2 li-bullet-0"><span class="c9 c0">We&rsquo;re already building towards this!</span></li></ul><ul class="c5 lst-kix_list_5-1 start"><li class="c10 c12 li-bullet-0"><span>The platform and SRE teams are discussing ways to enable federation of workload from edge to core. &nbsp;</span><span>Their specific use case centers around Kubernetes, but the idea transcends many other use cases. &nbsp;This whole proposal is designed to enable that exact thing but adds super-intelligence to it</span><span class="c1">.</span></li><li class="c10 c12 li-bullet-0"><span>UI team is working on turning the query/task/alerting UI into a notebook-style which indirectly supports the idea of using Dean&rsquo;s proposed ML UI work.</span></li><li class="c10 c12 li-bullet-0"><span>Telegraf team is talking about enabling Telegraf to more intelligently deal with data before it writes it to InfluxDB.</span></li></ul><ul class="c5 lst-kix_list_5-0"><li class="c2 li-bullet-0"><span>Given we&rsquo;re already building toward this (perhaps without knowing it) and we already have an edge presence, this gives us the ability to start adding value very soon. &nbsp;We can incrementally build toward what will be a much larger end goal starting with small but fruitful steps in the direction today.</span></li><li class="c2 li-bullet-0"><span>Telegraf&rsquo;s new execd processor plugin allows for massive extensibility of Telegraf </span></li></ul><p class="c3 c24"><span class="c1"></span></p><p class="c10"><span class="c0">How does this solve problems for </span><span class="c20 c0">specifically</span><span class="c9 c0">&nbsp;InfluxData?</span></p><ul class="c5 lst-kix_list_19-0 start"><li class="c2 li-bullet-0"><span class="c0">The </span><span class="c0">challenge of cardinality</span><span class="c0">&nbsp;is reduced/mitigated</span><span>: &nbsp;For most purposes -- in the context of the computations happening at the edge -- it is eliminated.</span></li><li class="c2 li-bullet-0"><span class="c0">Open source network monitoring is highly sought after: &nbsp;</span><span>Related to above, there is a well-known market ask for better open source / platform-based monitoring of networks. &nbsp;Currently, this type of monitoring is a hard problem to solve in the database world because of its natural cardinality. &nbsp;My hypothesis is that doing some of the per-IP / per-device -- which would have originally required large grouping operations on a larger cloud-based dataset -- could be done at the edge and drastically reduce cardinality of the central data store (the core).</span></li><li class="c2 li-bullet-0"><span class="c0">Competitive advantage overall: &nbsp;</span><span>Getting smarter about observability closer to data sources means less need for services like Prometheus, for instance, at the per-K8s-cluster level. &nbsp;In a way, this somewhat forces the market&rsquo;s hand toward the push-based model...which we typically tout as superior anyway.</span></li><li class="c2 li-bullet-0"><span class="c0">Telegraf&rsquo;s open output policy gives us more market control: &nbsp;</span><span>Telegraf outputs to other data sources. &nbsp;This includes many of our direct and even indirect customers. &nbsp;Adding value to the data </span><span class="c20">in</span><span>&nbsp;Telegraf gives the commercial control back to InfluxData and makes this inherent openness work to our advantage. </span></li><li class="c2 li-bullet-0"><span class="c0">Commercial preset solutions: </span><span>Given that edge-computing implies lack of attendance/attention toward the computations, people may not want to do much hands-on work in these systems. &nbsp;It seems foreseeable that there would be a market opportunity for commercial &ldquo;presets&rdquo; of computations. &nbsp;Since we would have control of the data collection as well as the processing, this seems plausible.</span></li><li class="c2 li-bullet-0"><span class="c0">Flux adoption: &nbsp;</span><span>If part or all of this story involved the use of Flux (say, embedded in Telegraf), the popularity of Telegraf could feasibly enable -- or even necessitate -- the adoption of the language, itself. &nbsp;</span></li><li class="c2 li-bullet-0"><span class="c0">Strategic control over the data pipeline</span><span>: The data pipeline is traditionally controlled by messaging and stream processing systems. &nbsp;I believe this point is one of the more pertinent ones, so it gets its own embedded writeup:</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c10 c11"><span class="c1">This writeup centers around where the value of the data lies and who controls it:</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span class="c1">Queueing systems and stream processing engines are getting more scalable, efficient, fault-tolerant, and smarter. &nbsp;Kafka, for instance, owns a plurality of queueing use cases and now Kafka Streams and KsqlDB have been added to glean insights from the data piping through the Kafka brokers themselves. Popular stream processing engines in the Apache ecosystem also work well with Kafka and many other popular queues like RabbitMQ and Pulsar. &nbsp;Kafka is not the only messaging system we should be thinking about.</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span class="c1">To narrow the scope of the idea, however, I will talk specifically about Kafka. &nbsp;In a large enterprise that is using Kafka as a distributed ledger for all events within a larger infrastructure &nbsp;(its real original purpose and where it has found its success), it only makes sense that additional tooling/APIs to complement it would be a good first place to look for extending functionality. &nbsp;Kafka Streams has enabled Kafka users to mung data as it sits in the queue. &nbsp;Applications can use this API to glean insights and provide real-time (essentially) evaluations of streaming data with which to make both human and programmatic decisions. &nbsp;Think auto-scaling events, alerts, etc. &nbsp;</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span class="c1">Now add in KsqlDB, an API that makes it easier to build applications that need to process streaming data. &nbsp;Applications can use the SQL language to query streams/tables directly or generate Materialized Views of tables and streams to be queried efficiently. &nbsp;Think of this as providing secondary indexing or, from the application&rsquo;s perspective, a cache that is generated at the storage level so you don&rsquo;t hit normal caching problems like data invalidation and certain race conditions.</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span>Given the above, it&rsquo;s not a far reach to think of how a UI like Grafana could integrate and provide real-time monitoring on top of Kafka, itself&hellip;.</span><span class="c20">instead</span><span class="c1">&nbsp;of a time series database.</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span class="c1">You might be thinking, but what about retention? &nbsp;That&rsquo;s where time series databases come in. We all know that time series data is where you want your historical time series data. &nbsp;That said, it&rsquo;s worth discussing the logical value of that.</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span>Let&rsquo;s assume, for a moment, that real-time monitoring is happening before data reaches the TSDB. &nbsp;What happens to time series data requirements after that? &nbsp;If the data has already been evaluated for alerts, real-time dashboards, early remediation, and real-time forecasting&hellip;.</span><span>the only stuff left for the TSDB is longer time range OLAP</span><span class="c1">. &nbsp;The TSDB is now used for things like capacity planning, machine learning / algo training, and other more ad hoc data science-y type analysis. &nbsp;</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span class="c1">What do all of the data science-y queries have in common when they&rsquo;re not implemented in real-time? &nbsp;They span long time ranges of data. &nbsp;The question then becomes, just how valuable is constant availability if the queried dataset is so large and doesn&rsquo;t need to be conducted in real-time? &nbsp;This is subjective to both the data scientist and the use case but I&rsquo;d argue, in general, not very. &nbsp;It&rsquo;s &ldquo;valuable&rdquo;, but is it valuable enough to procure software that moves the needle for our business? Our very own sales history supports this.</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c10 c11"><span class="c1">Table that question for a second and consider one more thing. &nbsp;Not only has the real-time value of the data left the building by the time the data reaches the TSDB, it&rsquo;s totally reasonable that the data the TSDB does see is significantly downsampled or filtered. &nbsp;Downsampled data is, as we know, less valuable per data point&hellip;.and fewer data points over all. &nbsp;This means less revenue for the TSDB vendor.</span></p><p class="c3"><span class="c1"></span></p><a id="id.3znysh7"></a><p class="c10"><span class="c26 c0">Proposal</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c1">This proposal is aimed at leveraging what we do best as a platform that both collects and stores time series data. &nbsp;The way I see combating this potential new threat to not just us but our entire space, is taking advantage of the popularity of Telegraf and its proximity to data sources to supersede/intercept the processing done on the streaming data. &nbsp;In other words, we would do the &ldquo;stream processing&rdquo; before the data touches streaming tools like Kafka, Flink, Spark, etc.</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c1">Currently, we support some very basic processing of data before it reaches the database. &nbsp;If InfluxData can support/enable/provide edge-processing of streaming data -- essentially at the agent level -- that handles the 95th-99th (arbitrary guess right now) percentile of use cases, we can retain the value of the data in our ecosystem. &nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span>You might now be thinking a) what would implementation look like, and/or b) how does doing this processing with </span><span class="c20">our</span><span>&nbsp;free software solve the revenue problem? &nbsp;Well, the answer to &ldquo;b&rdquo; is that we&rsquo;d likely look to monetize this in some way. &nbsp;Maybe just support (after all, this whole concept brings a lot more value to the edge, so support then becomes more necessary for our users). &nbsp;However, there are more and more requests for a clusterable (HA and auto-scaling) Telegraf. &nbsp;This could be offered commercially as well. &nbsp;</span><span>Telegraf, as an agent, doesn&rsquo;t lend itself to commercial high availability needs so much but Telegraf that is processing data and making data-driven decisions at the edge likely does.</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">Commercialization</span></p><p class="c10"><span>Touched on briefly in the section about how streaming engines can be competitive, </span><span>any edge processing could reduce the overall data being ingested into InfluxDB on premise or in the cloud</span><span class="c1">. &nbsp;The need to replace that revenue would need to be part of the overall strategy for this development. &nbsp;Any attempt to close source telegraf would likely be disastrous.</span></p><p class="c10"><span class="c1">Some ideas in this regard:</span></p><ul class="c5 lst-kix_list_15-0 start"><li class="c2 li-bullet-0"><span>License a component that works in conjunction with telegraf as a licenseable add-on to perform functions described in this proposal, or to allow only a small subset of the functionality as open source. </span></li><li class="c2 li-bullet-0"><span>License ML algorithms trained and tested beforehand that they can deploy to the edge.</span></li><li class="c2 li-bullet-0"><span>License the management/deployment of edge services</span></li></ul><p class="c3"><span class="c1"></span></p><a id="id.2et92p0"></a><p class="c10"><span class="c26 c0">My opinion on an MVP</span></p><p class="c3"><span class="c0 c26"></span></p><p class="c10"><span class="c0">First, here is a version of the envisioned architecture. &nbsp;</span><span>The below represents a &ldquo;fog-computing&rdquo; model. &nbsp;This represents the entirety of the infrastructure</span><span class="c9 c0">:</span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 609.50px; height: 565.89px;"><img alt="" src="images/image1.jpg" style="width: 609.50px; height: 565.89px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c9 c0"></span></p><p class="c3"><span class="c9 c0"></span></p><p class="c10"><span class="c9 c0">Next, we zoom into the site-level architecture:</span></p><p class="c3"><span class="c9 c0"></span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 550.67px;"><img alt="" src="images/image3.jpg" style="width: 624.00px; height: 550.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c9 c0"></span></p><p class="c10"><span class="c0">Lastly, we zoom into the device-level architecture. &nbsp;</span><span class="c1">This whole proposal really centers on this part. &nbsp;This is the edge:</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 509.33px;"><img alt="" src="images/image2.jpg" style="width: 624.00px; height: 509.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">30,000 Feet</span></p><p class="c10"><span class="c1">To have control over processing at the edge -- and using our current tooling -- Telegraf and/or Flux need to support at least a large subset of the types of processing people are doing in their data [streaming] pipelines as well as point-in-time decision making.</span></p><ul class="c5 lst-kix_list_2-0 start"><li class="c2 li-bullet-0"><span>It would have to support correlation between data streams -- something not done in point-in-time alerting systems like Sensu/Nagios.</span></li><li class="c2 li-bullet-0"><span>It would also have to do &ldquo;a lot&rdquo; (&ldquo;a lot&rdquo; is TBD) of what traditional stream processing tooling does, and be intelligent...all while being easier to work with and maintain. &nbsp;If Telegraf can do p85 of what Spark, Flink, and Kafka Streams can do, that could be all we need to regain control of the value of the data.</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c1">With some extended functionality of the API, we could consider bi-directional processing and traffic directing. If telegraf is processing at the edge, couldn&rsquo;t it also send its evaluations back to edge systems for consumption? This would look more like the control loop of OSI PI. &nbsp;This may be more of a reach goal but a good one to keep in mind! </span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c1">Ultimately, the model proposed is flexible. &nbsp;As the University of Minnesota paper suggests: </span></p><p class="c3"><span class="c1"></span></p><p class="c10 c35 c24"><span class="c20 c21">...a geodistributed analytics system must determine how much computation to perform at the edges, and how much to leave for the center (i.e., where to compute), as well as when to send partial results from edges to the center.</span></p><p class="c3 c24 c35"><span class="c21 c20"></span></p><p class="c10"><span class="c1">The level of control we give the user of this model is an implementation detail but suffice it to say that we have options.</span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c0 c9">10,000 Feet</span></p><p class="c10"><span class="c1">The high level MVP (just a hypothesis for now) for edge processing needs to support:</span></p><ul class="c5 lst-kix_list_6-0 start"><li class="c2 li-bullet-0"><span>Point-in-time event data evaluation</span></li></ul><ul class="c5 lst-kix_list_6-1 start"><li class="c10 c12 li-bullet-0"><span>Conditional routing of metrics/events based on said evaluations; routing would include:</span></li></ul><ul class="c5 lst-kix_list_6-2 start"><li class="c10 c13 li-bullet-0"><span>Other data stores at entirely separate addresses</span></li><li class="c10 c13 li-bullet-0"><span>Other &ldquo;buckets&rdquo; within &lt;data store&gt; -- supports dynamic downsampling as well</span></li></ul><ul class="c5 lst-kix_list_6-0"><li class="c2 li-bullet-0"><span>Windowing -- sliding and tumbling</span></li><li class="c2 li-bullet-0"><span>Stream joins -- or at least no logical partitioning of &ldquo;streams&rdquo; -- required for correlations </span></li></ul><p class="c10"><span class="c1">Nice-to-haves:</span></p><ul class="c5 lst-kix_list_13-0 start"><li class="c2 li-bullet-0"><span>Dynamic aggregation/downsampling or aggregation disabling. &nbsp;Disabling aggregation when &lt;process&gt; evaluates to &lt;result&gt; so that raw data can be retained for longer is something that has been requested. &nbsp;More detail in 1,000 foot view below.</span></li></ul><p class="c3"><span class="c9 c0"></span></p><p class="c10"><span class="c9 c0">1,000 Feet</span></p><p class="c10"><span class="c1">MVP:</span></p><ul class="c5 lst-kix_list_3-0 start"><li class="c2 li-bullet-0"><span>Rate, difference, percentile, forecasting (triple exponential moving average, etc.),</span></li></ul><ul class="c5 lst-kix_list_3-1 start"><li class="c10 c12 li-bullet-0"><span>Requires holding state for &lt;time period&gt;</span></li><li class="c10 c12 li-bullet-0"><span>Can use t-digest (and/or other algorithms) to estimate values over time slices</span></li></ul><ul class="c5 lst-kix_list_3-2 start"><li class="c10 c13 li-bullet-0"><span>I&rsquo;m assuming it&rsquo;s desirable to have control over the sample size of the data to be computed</span></li></ul><ul class="c5 lst-kix_list_3-0"><li class="c2 li-bullet-0"><span>Support for conditionals and/or loops/maps</span></li><li class="c2 li-bullet-0"><span>Support for true streaming -- not </span><span class="c20">only</span><span>&nbsp;micro-batching</span></li></ul><ul class="c5 lst-kix_list_3-1 start"><li class="c10 c12 li-bullet-0"><span>Micro-batching places at least some burden on the asset</span></li><li class="c10 c12 li-bullet-0"><span>Micro-batching is an inefficient way of dealing with point-in-time metrics/events</span></li></ul><ul class="c5 lst-kix_list_3-0"><li class="c2 li-bullet-0"><span>If Telegraf, I imagine Telegraf will need better support for DAG-like pipelines between inputs, parsers/processors, aggregators, and outputs</span></li></ul><ul class="c5 lst-kix_list_3-1 start"><li class="c10 c12 li-bullet-0"><span>Currently, Telegraf supports only namepass/namedrop, tagpass/tagdrop, fieldpass/fielddrop, which is a rigid and inefficient way of routing &ldquo;jobs&rdquo;</span></li></ul><p class="c10"><span class="c1">&nbsp;</span></p><p class="c10"><span class="c1">Nice-to-haves:</span></p><ul class="c5 lst-kix_list_16-0 start"><li class="c2 li-bullet-0"><span>RE dynamically disabling aggregations: this is a desired capability when users are assumed to be pre-aggregating and retaining data at different levels of precision for different retention periods. &nbsp;The difference is, in certain cases (incidents or periods of high utilization), users will want to know what was happening during that time </span><span class="c20">precisely</span><span>. &nbsp;If they want to look back at that information at a date that outlives the &ldquo;raw&rdquo; dataset&rsquo;s TTL, they can&rsquo;t unless they have it stored in a &ldquo;bucket&rdquo; with longer retention. &nbsp;Storing raw data forever is expensive in more ways than one. &nbsp;Dynamically disabling pre-aggregation can allow for storing raw precision data </span><span class="c20">only</span><span>&nbsp;in &ldquo;patches&rdquo; during periods of interest.</span></li></ul><p class="c10"><span class="c1">Specific functionality:</span></p><ul class="c5 lst-kix_list_11-0 start"><li class="c2 li-bullet-0"><span>Alert on certain values</span></li></ul><ul class="c5 lst-kix_list_11-1 start"><li class="c10 c12 li-bullet-0"><span>Logs</span></li></ul><ul class="c5 lst-kix_list_11-2 start"><li class="c10 c13 li-bullet-0"><span>Level == &lt;level&gt;</span></li><li class="c10 c13 li-bullet-0"><span>Message == &lt;message&gt;</span></li></ul><ul class="c5 lst-kix_list_11-1"><li class="c10 c12 li-bullet-0"><span>Metric value is &gt;, &lt;, or == &lt;value&gt;</span></li></ul><ul class="c5 lst-kix_list_11-0"><li class="c2 li-bullet-0"><span>Alerts on:</span></li></ul><ul class="c5 lst-kix_list_11-1 start"><li class="c10 c12 li-bullet-0"><span>Short-window evaluations for thresholds/deadmans</span></li><li class="c10 c12 li-bullet-0"><span>T-digest (et al) estimations of percentiles/quantiles (if doesn&rsquo;t exist on the device/app itself already)</span></li><li class="c10 c12 li-bullet-0"><span>Rates</span></li></ul><ul class="c5 lst-kix_list_11-0"><li class="c2 li-bullet-0"><span>Embedding scripting (Starlark, MicroPython, Flux, Lua, etc.) to trigger other processes that are not just alerts</span></li></ul><ul class="c5 lst-kix_list_11-1 start"><li class="c10 c12 li-bullet-0"><span>Auto-remediation (may not directly control device but could be an interface that enables this)</span></li><li class="c10 c12 li-bullet-0"><span>Auto-scaling </span></li></ul><ul class="c5 lst-kix_list_11-2 start"><li class="c10 c13 li-bullet-0"><span>K8s metrics server endpoint as an infrastructure example</span></li></ul><ul class="c5 lst-kix_list_11-0"><li class="c2 li-bullet-0"><span>Support for UDFs / custom models</span></li></ul><ul class="c5 lst-kix_list_11-1 start"><li class="c10 c12 li-bullet-0"><span>Cloud computing will still be important as AI models will be trained on larger infrastructure. &nbsp;Those models should be deployable and easily pluggable here.</span></li><li class="c10 c12 li-bullet-0"><span>Fog computing as well</span></li></ul><ul class="c5 lst-kix_list_11-2 start"><li class="c10 c13 li-bullet-0"><span>This is where I see InfluxDB OSS playing a major role!</span></li></ul><ul class="c5 lst-kix_list_11-0"><li class="c2 li-bullet-0"><span>Some form of series-/stream-joining</span></li></ul><ul class="c5 lst-kix_list_11-1 start"><li class="c10 c12 li-bullet-0"><span>This enables correlations to be drawn in real-time</span></li><li class="c10 c12 li-bullet-0"><span>Collect hardware-level, kernel-level, system-level, [virtualization-level,] and application-level metrics, traces, and logs, enabling the edge to draw causality/correlations</span></li></ul><ul class="c5 lst-kix_list_11-2 start"><li class="c10 c13 li-bullet-0"><span>This might be the real North Star</span></li></ul><ul class="c5 lst-kix_list_11-0"><li class="c2 li-bullet-0"><span>Update self with new AI models if desired</span></li></ul><ul class="c5 lst-kix_list_11-1 start"><li class="c10 c12 li-bullet-0"><span>Telegraf can already pull config from InfluxDB. &nbsp;With an instance of InfluxDB deployed per &ldquo;site&rdquo;, the Telegrafs/&ldquo;edge-computing nodes&rdquo; can regularly pull new configuration -- which has been updated with a new model written in the embedded configuration language -- and essentially learn on its own this way.</span></li><li class="c10 c12 li-bullet-0"><span>Could be used for better anomaly detection, perhaps more intelligent trace sampling (?), etc.</span></li></ul><p class="c3 c27"><span class="c1"></span></p><p class="c10"><span class="c9 c0">Possible capabilities:</span></p><ul class="c5 lst-kix_list_1-0 start"><li class="c2 li-bullet-0"><span class="c1">Easy streaming statistical analysis</span></li></ul><ul class="c5 lst-kix_list_1-1 start"><li class="c10 c12 li-bullet-0"><span>Stddev, variance, etc.</span></li><li class="c10 c12 li-bullet-0"><span>Very little state is required to run these on streaming data so estimating may not even be necessary</span></li></ul><ul class="c5 lst-kix_list_1-0"><li class="c2 li-bullet-0"><span>Anomaly detection</span></li></ul><ul class="c5 lst-kix_list_1-1 start"><li class="c10 c12 li-bullet-0"><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://towardsdatascience.com/introduction-to-matrix-profiles-5568f3375d90&amp;sa=D&amp;source=editors&amp;ust=1691727433134389&amp;usg=AOvVaw20chlVC45n73ivvrYIKGV9">Time series Matrix Profiles</a></span></li></ul><ul class="c5 lst-kix_list_1-2 start"><li class="c10 c13 li-bullet-0"><span>If can hold state of &ldquo;full time series&rdquo; (for a certain period -- say a day or a week) and compare sliding windows to that to generate Distance Profiles, these distances can be used in alerting...or they can be metrics emitted either in addition to or in lieu of the raw data&hellip;.or both.</span></li><li class="c10 c13 li-bullet-0"><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://www.cs.ucr.edu/~eamonn/SCRIMP_ICDM_camera_ready_updated.pdf&amp;sa=D&amp;source=editors&amp;ust=1691727433135016&amp;usg=AOvVaw3yNlHjNgU6Dnn8aryBJk38">Matrix Profile estimation</a></span><span>&nbsp;can be done if needed. &nbsp;Python library STUMPY provides either.</span></li></ul><ul class="c5 lst-kix_list_1-1"><li class="c10 c12 li-bullet-0"><span>Applications of summarization (below)</span></li></ul><ul class="c5 lst-kix_list_1-0"><li class="c2 li-bullet-0"><span class="c1">Summarization sketches for statistical analysis that would otherwise require larger samples and time windows</span></li></ul><ul class="c5 lst-kix_list_1-1 start"><li class="c10 c12 li-bullet-0"><span>T-digest</span></li><li class="c10 c12 li-bullet-0"><span>HyperLogLog count sketches</span></li></ul><ul class="c5 lst-kix_list_1-0"><li class="c2 li-bullet-0"><span>Series aggregation</span></li></ul><ul class="c5 lst-kix_list_1-1 start"><li class="c10 c12 li-bullet-0"><span>In addition to simple aggregations on values for an individual series (&ldquo;metric&rdquo; for all intents and purposes), grouping by keys to make aggregations multi-dimensional.</span></li></ul><ul class="c5 lst-kix_list_1-0"><li class="c2 li-bullet-0"><span>Determine on-the-fly discretization of features and send signal of new groupings so the core engine can make the necessary change</span></li></ul><ul class="c5 lst-kix_list_1-1 start"><li class="c10 c12 li-bullet-0"><span>Is this something that could integrate with the Spark API, for instance?</span></li></ul><ul class="c5 lst-kix_list_1-0"><li class="c2 li-bullet-0"><span>Conditional routing/downsampling</span></li></ul><ul class="c5 lst-kix_list_1-1 start"><li class="c10 c12 li-bullet-0"><span>If &lt;evaluation result&gt; send alert to &lt;handler&gt;</span></li></ul><ul class="c5 lst-kix_list_1-2 start"><li class="c10 c13 li-bullet-0"><span>Log alert and forward on for storing/processing downstream</span></li></ul><ul class="c5 lst-kix_list_1-1"><li class="c10 c12 li-bullet-0"><span>Keys with &lt;value&gt;/&lt;type&gt; go to &lt;output&gt;</span></li></ul><ul class="c5 lst-kix_list_1-2 start"><li class="c10 c13 li-bullet-0"><span>Log event to downstream</span></li></ul><ul class="c5 lst-kix_list_1-1"><li class="c10 c12 li-bullet-0"><span>Keys with &lt;value&gt;/&lt;type&gt; go to &lt;processor&gt;</span></li></ul><ul class="c5 lst-kix_list_1-2 start"><li class="c10 c13 li-bullet-0"><span>Log event to downstream</span></li></ul><ul class="c5 lst-kix_list_1-1"><li class="c10 c12 li-bullet-0"><span>If &lt;evaluation result&gt; send to &lt;output&gt; / &lt;processor&gt;</span></li></ul><ul class="c5 lst-kix_list_1-2 start"><li class="c10 c13 li-bullet-0"><span>Log event to downstream</span></li></ul><ul class="c5 lst-kix_list_1-0"><li class="c2 li-bullet-0"><span>Transfer Learning</span></li></ul><ul class="c5 lst-kix_list_1-1 start"><li class="c10 c12 li-bullet-0"><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://machinelearningmastery.com/transfer-learning-for-deep-learning/%23:~:text%3DTransfer%2520learning%2520is%2520a%2520machine,model%2520on%2520a%2520second%2520task&amp;sa=D&amp;source=editors&amp;ust=1691727433137449&amp;usg=AOvVaw14NcBOalVSal6wepZ3Fsfl">https://machinelearningmastery.com/transfer-learning-for-deep-learning/#:~:text=Transfer%20learning%20is%20a%20machine,model%20on%20a%20second%20task</a></span></li><li class="c10 c12 li-bullet-0"><span>https://www.semanticscholar.org/paper/A-Survey-on-Transfer-Learning-Pan-Yang/a25fbcbbae1e8f79c4360d26aa11a3abf1a11972?p2df</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c1">Profiles:</span></p><ul class="c5 lst-kix_list_20-0 start"><li class="c2 li-bullet-0"><span>Alert on a specific event in real-time</span></li></ul><ul class="c5 lst-kix_list_20-1 start"><li class="c10 c12 li-bullet-0"><span>Rather than sampling every 10s, 30s, etc., sample continuously across a time window and alert specific values as they occur</span></li><li class="c10 c12 li-bullet-0"><span>The alert having happened is what the core would see, but the user wouldn&rsquo;t be waiting for the alert to happen from the core.</span></li></ul><ul class="c5 lst-kix_list_20-0"><li class="c2 li-bullet-0"><span>Alert on string of events in near-real-time</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">Data we still need to validate this proposal:</span></p><ul class="c5 lst-kix_list_4-0 start"><li class="c2 li-bullet-0"><span>What are common and perhaps uncommon-but-powerful stream processing operations happening today on time series data?</span></li><li class="c2 li-bullet-0"><span>What, out of the above, can feasibly be brought to the edge (smaller windows and sample sizes...and lower resource allocation)</span></li><li class="c2 li-bullet-0"><span>General gauge on how popular this idea would be? &nbsp;Also, market reception to its viability -- just because it is viable, doesn&rsquo;t mean the market will give it a chance. &nbsp;Things can be built too early.</span></li></ul><p class="c10"><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">Problems to solve:</span></p><ul class="c5 lst-kix_list_18-0 start"><li class="c2 li-bullet-0"><span class="c1">Data path&rsquo;s lack of awareness of stochastic changes to time series due to external influence</span></li></ul><ul class="c5 lst-kix_list_18-1 start"><li class="c10 c12 li-bullet-0"><span>If configuration of a system is changed, how is that recorded as accompanying information with a resulting change in the time series data about that system?</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c10"><span class="c9 c0">Links:</span></p><p class="c10"><span>Value of edge analytics: </span><span class="c4"><a class="c16" href="https://www.google.com/url?q=https://www.embedded-computing.com/guest-blogs/edge-analytics-complementing-cloud-computing&amp;sa=D&amp;source=editors&amp;ust=1691727433139926&amp;usg=AOvVaw1pFG2fSwGyCrxo4Zz4MPR6">https://www.embedded-computing.com/guest-blogs/edge-analytics-complementing-cloud-computing</a></span></p><p class="c3"><span class="c1"></span></p></body></html>